<!DOCTYPE html>
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<title>NLP papers</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<div id="Natural Language Precessing papers and notes | [[/index|home]]"><h1 id="Natural Language Precessing papers and notes | [[/index|home]]" class="header"><a href="#Natural Language Precessing papers and notes | [[/index|home]]">Natural Language Precessing papers and notes | <a href="index.html">home</a></a></h1></div>
<ul>
<li class="done0">
<a href="papers/Miller - 1995 - WordNet a lexical database for English.html">Miller - 1995 - WordNet a lexical database for English</a>: WordNet

<li class="done0">
<a href="papers/Mikolov et l.html">Mikolov et l. - 2013 - Distributed Representations of Words and Phrases and their Compositionality</a>

<li class="done0">
<a href="papers/Rong - 2016 - word2vec Parameter Learning Explained.html">Rong - 2016 - word2vec Parameter Learning Explained</a>: word2vec

<li class="done4">
<a href="papers/Vaswani et al - 2017 - Attention Is All You Need.html">Vaswani et al - 2017 - Attention Is All You Need</a>: Attention/Transformer

<li class="done0">
<a href="papers/Peters et al - 2018 - Deep contextualized word representations.html">Peters et al - 2018 - Deep contextualized word representations</a>: ElMo

<li class="done0">
<a href="papers/Radford et al -2018 - Improving Language Understanding by Generative Pre-Training.html">Radford et al -2018 - Improving Language Understanding by Generative Pre-Training</a>: GPTv1

<li class="done2">
<a href="papers/Devlin et al - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.html">Devlin et al - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding</a>: BERT

</ul>

</body>
</html>
