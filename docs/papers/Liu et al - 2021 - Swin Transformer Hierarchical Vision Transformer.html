<!DOCTYPE html>
<html>
<head>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="Stylesheet" type="text/css" href="../style.css">
<title>Liu et al - 2021 - Swin Transformer Hierarchical Vision Transformer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>

<div id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]"><h1 id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]" class="header"><a href="#Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | <a href="../index.html">home</a></a></h1></div>

<ul>
<li>
Useful Items

<ul>
<li>
<a href="https://arxiv.org/abs/2103.14030">arXiv Paper URL</a>

<li>
<a href="https://github.com/microsoft/Swin-Transformer">Github Code Repository</a>

<li>
<a href="https://www.connectedpapers.com/main/c8b25fab5608c3e033d34b4483ec47e68ba109b7/Swin-Transformer%3A-Hierarchical-Vision-Transformer-using-Shifted-Windows/graph">Connected Papers URL</a>

</ul>
</ul>
 
<ul>
<li>
Storage

<ul>
<li>
<a href="http://cdn.emm.ink/papers/CV/transformer/Liu%20et%20al.%20-%202021%20-%20Swin%20Transformer%20Hierarchical%20Vision%20Transformer%20.pdf">Cloud Storage</a>

<li>
<a href="/Volumes/Mac OS X Data/Zotero/storage/9UI8KNFB/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf">Local Storage</a>

</ul>
</ul>

 
<div id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas"><h4 id="Key Ideas" class="header"><a href="#Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas">Key Ideas</a></h4></div>
 
<div id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Overall Architecture"><h5 id="Overall Architecture" class="header"><a href="#Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Overall Architecture">Overall Architecture</a></h5></div>
<p>
<img src="http:cdn.emm.ink/pics/Swin3.png" alt="The architecture of Swin Transformer" style="height:280px;width:680px;" />
<img src="http:cdn.emm.ink/pics/Swin5.png" alt="Detailed architecture specifications" style="height:280px;width:680px;" />
</p>
<ul>
<li>
Stage 1: Linear projection of flatterned non-overlapping patches as their respective token/feature, then feed these tokens added with position embedding to first two successive Swin Transformer Blocks

<li>
Stage 2&amp;3&amp;4: The first patch merging layer concatenates the features of each group of \( 2 \times 2 \) neighboring patches, then applies a linear layer on the concatated features to downsample feature channels 2 times, and last feed these processed features to stacked two consecutive Swin Transformer Blocks

</ul>
 
<div id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Main Characteristics"><h5 id="Main Characteristics" class="header"><a href="#Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Main Characteristics">Main Characteristics</a></h5></div>
<p>
<img src="http:cdn.emm.ink/pics/Swin10.png" alt="Swin Transformer characteristics vs ViT" style="height:200px;width:400px;margin-left:500px;" />
</p>
<ul>
<li>
<span id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Main Characteristics-hierarchical feature maps"></span><strong id="hierarchical feature maps">hierarchical feature maps</strong> by <span id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Main Characteristics-merging image patches in deeper layers"></span><strong id="merging image patches in deeper layers">merging image patches in deeper layers</strong>

<li>
<span id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Main Characteristics-linear computation complexity to input image size"></span><strong id="linear computation complexity to input image size">linear computation complexity to input image size</strong> due to <span id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Main Characteristics-computation of self-attention only within each local window"></span><strong id="computation of self-attention only within each local window">computation of self-attention only within each local window</strong>

</ul>
   
<div id="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Well-designed Details"><h5 id="Well-designed Details" class="header"><a href="#Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [[/index|home]]-Key Ideas-Well-designed Details">Well-designed Details</a></h5></div>
<ul>
<li>
window multi-head self attention(W-MSA)

<ul>
<li>
W-MSA for linear computation complexity to image size
\begin{align}
&amp;\Omega(MSA)=4hwC^2 + 2(hw)^2C \\
&amp;\Omega(W\mbox{-}MSA)=4hwC^2 + 2M^2hwC \\
\end{align}

</ul>
</ul>
 
<ul>
<li>
shifted window multi-head self attention(SW-MSA)

<ul>
<li>
SW-MSA for augement W-MSA by introducing connections between neighboring non-overlapping windows in the previous layer

</ul>
</ul>
<p>
<img src="http:cdn.emm.ink/pics/Swin9.png" alt="Shifted windows approach" style="height:200px;margin-left:450px;" />  
</p>
\begin{align}
&amp;\hat{z}^l=W\mbox{-}MSA(LN(z^{l-1}))+z^{l-1} \\
&amp;z^l=MLP(LN(\hat{z}^l))+\hat{z}^l \\
&amp;\hat{z}^{l+1}=SW\mbox{-}MSA(LN(z^l))+z^l \\
&amp;z^{l+1}=MLP(LN(\hat{z}^{l+1}))+\hat{z}^{l+1} \\
\end{align}
 
<ul>
<li>
cyclic-shifting toward the top-left direction

<ul>
<li>
a batched window may be composed of several sub-windows that are not adjacent in the feature map, so a masking mechanism is employed to limit self-attention computation to within each sub-window

</ul>
</ul>
<p>
<img src="http:cdn.emm.ink/pics/Swin8.png" alt="Cycylic shift for efficient batch computation" style="height:200px;margin-left:350px;" />
</p>
 
<ul>
<li>
relative position bias
\[
Attention(Q,K,V)=SoftMax(QK^T/\sqrt{d}+B)V
\]
  where \( Q, K, V\in \mathbb{R}^{M^2\times d} \) are the query, key and value matrices; d is the query/key dimension, and \( M^2 \) is the number of patches in a window

</ul>

</body>
</html>
